{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 005 Real Estate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retrieve the dataste from https://www.kaggle.com/datasets/mrdaniilak/russia-real-estate-20182021.\n",
    "\n",
    "If using the below cell, make sure you have a Kaggle API token in a `kaggle.json` file in `~/.kaggle/`. Otherwise, please download the data manually and place it under a folder `./data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !kaggle datasets download mrdaniilak/russia-real-estate-20182021\n",
    "# !mv ./russia-real-estate-20182021.zip ./data/russia-real-estate-20182021.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('./data/russia-real-estate-20182021.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we may proceed with exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV_PATH = './data/all_v2.csv'\n",
    "\n",
    "# load csv data\n",
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "# remove duplicate data\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this Kaggle Dataset was originally pulled from GeoNames (http://www.geonames.org/), which has its own \"regions\" separated by county. Our city of interest, Moscow, has ID 81.\n",
    "\n",
    "Addtionally, we will give categorical data appropriate labels given by the dataset spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOSCOW_CODE = 81\n",
    "MAX_NUM_SAMPLES = 100000\n",
    "SEED = 69\n",
    "\n",
    "moscow_df = df.loc[df['region'] == MOSCOW_CODE]\n",
    "moscow_df = moscow_df.drop(['time', 'geo_lat', 'geo_lon', 'region'], axis=1)\n",
    "moscow_df['date'] = moscow_df['date'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "moscow_df['object_type'] = moscow_df['object_type'].replace(1, 'preowned').replace(11, 'new')\n",
    "moscow_df['building_type'] = moscow_df['building_type'].replace(0, 'other').replace(1, 'panel').replace(2, 'monolithic').replace(3, 'brick').replace(4, 'blocky').replace(5, 'wooden')\n",
    "\n",
    "# -1 means studio apartment, so we replace with 0 (since studio apartments have no extra rooms)\n",
    "# there are not other datapoints with value 0\n",
    "moscow_df['rooms'] = moscow_df['rooms'].replace(-1, 0)\n",
    "\n",
    "# remove rows with errorneous data\n",
    "moscow_df = moscow_df[moscow_df['price'] >= 0]\n",
    "moscow_df = moscow_df[moscow_df['rooms'] >= 0]\n",
    "moscow_df['price'] = moscow_df['price'] * 1e-4\n",
    "\n",
    "# cap number of elements\n",
    "moscow_df = moscow_df.sample(MAX_NUM_SAMPLES, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at the data distributions for the ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column is skewed right, meaning we have some extreme outliers for each column. This is because in the real estate market, while most \"normal\" places have a similar price, the price ceiling for real estate can be very high. While these outliers are sparse, they could still bias our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot each variable against price to look for possible correlations. We will only looks at data points with price less than $2 \\cdot 10^7$ to get better plots by removing price outliers. We will also plot regression lines for each to quantify per-variable correlation strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "def calc_R2(x, y, ax=None, **kwargs):\n",
    "    ax = ax or plt.gca()\n",
    "    _, _, r_value, _, _ = linregress(x=x, y=y)\n",
    "    ax.annotate(f'$R^2 = {r_value ** 2:.2f}$', xy=(.05, 1), xycoords=ax.transAxes, fontsize=8, ha='left', va='top')\n",
    "\n",
    "g = sns.pairplot(moscow_df.loc[moscow_df['price'] < 2e7].sample(1000), kind='reg', y_vars=['price'], plot_kws={'line_kws':{'color':'red'}})\n",
    "\n",
    "g.map_upper(calc_R2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and level have near-zero correlations. The number of levels in the building (i.e. building size) and number of rooms, as well have kitchen area, seem like te might have some signidicance. The most important (single) variable seems to be area. However, even here we don't have a strong correlation. Hopefully combining these variables into a multivariate regression will lead to stronger correlation.\n",
    "\n",
    "Additionally, all of the correlations seem to be closest to linear (as opposed to some polynomial fit). So, a polynomial regression may not perform better than a linear regression.\n",
    "\n",
    "However, in the above we only use about 1000 samples (for efficiency). We can see these correlation results across the dataset more easily with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(moscow_df.drop(['building_type', 'object_type'], axis=1).corr(), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, `date` and `level` have a stronger correlation than from our 1000 samples, but each individual variable still does not have a strong enough correlation for prediction.\n",
    "\n",
    "Thus, we proceed to fitting some models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = moscow_df[['date', 'building_type', 'level', 'levels', 'rooms', 'area', 'kitchen_area', 'object_type']]\n",
    "y = moscow_df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "print(f'{len(y_train)} train samples; {len(y_test)} test samples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will one-hot the categorical data using sklearn's one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for col in ['building_type', 'object_type']:\n",
    "\n",
    "    one_hot = OneHotEncoder()\n",
    "    one_hot.fit(X_train[[col]])\n",
    "\n",
    "    X_train.loc[:, one_hot.categories_[0]] = one_hot.transform(X_train[[col]]).todense()\n",
    "    X_test.loc[:, one_hot.categories_[0]] = one_hot.transform(X_test[[col]]).todense()\n",
    "\n",
    "    X_train = X_train.drop(col, axis=1)\n",
    "    X_test = X_test.drop(col, axis=1)\n",
    "\n",
    "X_train.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will scale the data. Note that not all models require scaled data; while models like linear regression require scaling to avoid overemphasis of certain datapoints, models like descision trees (and, by extension, random forests) are not affected by unscaled variables.\n",
    "\n",
    "It is also important to note that scaled data won't *negatively* impact descision trees, but we make a copy of scaled data to more explicitly demonstrate model understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ordinal_cols = ['date', 'level', 'levels', 'rooms', 'area', 'kitchen_area']\n",
    "X_train_ordinal, X_test_ordinal = X_train[ordinal_cols], X_test[ordinal_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_ordinal)\n",
    "\n",
    "X_train_scaled, X_test_scaled = X_train, X_test\n",
    "\n",
    "X_train_scaled.loc[:, scaler.feature_names_in_] = scaler.transform(X_train_ordinal)\n",
    "X_test_scaled.loc[:, scaler.feature_names_in_] = scaler.transform(X_test_ordinal)\n",
    "\n",
    "X_train_scaled.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_EPOCHS = 1000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SckiKit doesn't offer MAE loss for its `LinearRegression` implementation. However, `linear_model.SGDRegressor` can be made to use an MAE loss by using `epsilon_insensitive` loss and setting its $\\epsilon$ hyperparam to $0$.\n",
    "\n",
    "Note that this does mean we will be using SGD for descent, so our model will take more steps to converge and loss won't strcitly decrease, but in our case using SGD will actually be helpful since our dataset is relatively large, so each epoch will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# linreg_mae_model = SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=MAX_EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linreg_mae_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = 'trained_model.sav'\n",
    "# pickle.dump(linreg_mae_model, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealEstateDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, train=True, max_cache_size=800000):\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "\n",
    "        self.X = X_train if self.train else X_test\n",
    "        self.y = y_train if self.train else y_test\n",
    "\n",
    "        self.cache = dict()\n",
    "        self.max_cache_size = max_cache_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if index in self.cache.keys():\n",
    "            return self.cache[index]\n",
    "\n",
    "        X_sample = torch.tensor(self.X[index]).to(torch.float32).squeeze()\n",
    "        y_sample = torch.tensor(self.y[index]).to(torch.float32).squeeze()\n",
    "\n",
    "        if len(self.cache) >= self.max_cache_size:\n",
    "            self.cache.popitem()\n",
    "\n",
    "        self.cache[index] = (X_sample, y_sample)\n",
    "\n",
    "        return self.cache[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RealEstateDataset(X_train_scaled.to_numpy(), y_train.to_numpy(), X_test_scaled.to_numpy(), y_test.to_numpy(), train=True)\n",
    "test_dataset = RealEstateDataset(X_train_scaled.to_numpy(), y_train.to_numpy(), X_test_scaled.to_numpy(), y_test.to_numpy(), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDNN(nn.Module):\n",
    "    def __init__(self, input_size, fcs=[24, 12, 6, 1]):\n",
    "        super(RegressionDNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.fcs = fcs\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._make_layers()\n",
    "        )\n",
    "\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def _make_layers(self):\n",
    "        layers = [nn.Linear(self.input_size, self.fcs[0])]\n",
    "\n",
    "        for i in range(len(self.fcs) - 1):\n",
    "            # layers.append(nn.BatchNorm1d(self.fcs[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(self.fcs[i], self.fcs[i+1]))\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5000\n",
    "DATA_DIMS = train_dataset[0][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "def save(model, optimizer, save_path='model_checkpoint.pt'):\n",
    "\n",
    "    save_dict = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "def load(model, optimizer, load_path='model_checkpoint.pt'):\n",
    "\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "def train(train_dl, test_dl, epochs=EPOCHS, batch_size=BATCH_SIZE, input_size=DATA_DIMS, print_batch_every=None,\n",
    "          lr=1e-4, momentum=0.9,\n",
    "          checkpoint_dir='./checkpoints', pretrained_path=None,\n",
    "          logging = False, log_init = False, project_name='Russian-Real-Estate-Regression', group_name='DNN', run_name=None,\n",
    "          model_fcs=None):\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    if logging and log_init:\n",
    "        wandb.init(project=project_name, group=group_name, name=run_name, config=dict(\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            fcs = str(model_fcs),\n",
    "            lr = lr,\n",
    "            momentum = momentum,\n",
    "        ))\n",
    "\n",
    "    model = RegressionDNN(input_size, fcs=model_fcs).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        model, optimizer = load(model, optimizer, load_path=pretrained_path)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        batch = 0\n",
    "        train_loss = 0\n",
    "        for data in iter(train_dl):\n",
    "            batch += 1\n",
    "\n",
    "            X, y = data\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            loss = F.l1_loss(pred, y, reduction='sum')\n",
    "            train_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_batch_every is not None and ((batch-1) % print_batch_every == 0):\n",
    "                print(f'epoch: {epoch}\\tbatch: {batch}/{len(train_dl)}\\ttrain_loss: {loss.item()}', file=sys.stderr)\n",
    "\n",
    "        train_loss /= len(train_dl.dataset)\n",
    "    \n",
    "        # print(f'epoch: {epoch}\\ttrain_loss: {train_loss}', file=sys.stderr)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch = 0\n",
    "            test_loss = 0\n",
    "            for data in iter(test_dl):\n",
    "                batch += 1\n",
    "\n",
    "                X, y = data\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                pred = model(X)\n",
    "\n",
    "                loss = F.l1_loss(pred, y, reduction='sum')\n",
    "                test_loss += loss\n",
    "\n",
    "                if print_batch_every is not None and ((batch-1) % print_batch_every == 0):\n",
    "                    print(f'epoch: {epoch}\\tbatch: {batch}/{len(test_dl)}\\ttest_loss: {loss.item()}', file=sys.stderr)\n",
    "\n",
    "            test_loss /= len(test_dl.dataset)\n",
    "        \n",
    "            # print(f'epoch: {epoch}\\ttest_loss: {test_loss}', file=sys.stderr)\n",
    "\n",
    "        save_path = Path(checkpoint_dir) / Path(f'reg_model_{epoch}.pt')\n",
    "        save(model, optimizer, save_path=str(save_path))\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if logging:\n",
    "            wandb.log({ 'train/loss': train_loss, 'test/loss': test_loss })\n",
    "\n",
    "    if logging:\n",
    "        wandb.finish()\n",
    "\n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "trained_model, train_losses, test_losses = train(train_dl, test_dl,\n",
    "                                        logging=True, log_init=True, group_name='DNN-Training', run_name=f'lr_{lr}',\n",
    "                                        print_batch_every=None, checkpoint_dir=f'./checkpoints/train_lr_{lr}',\n",
    "                                        model_fcs=[128, 256, 256, 256, 1], lr=1e-5, momentum=0.9, epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs118a_g5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
